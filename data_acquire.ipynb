{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import urllib.request as ur\n",
    "\n",
    "##########-------------------------------------------------------------------------------------##########\n",
    "##########-------------------------------------------------------------------------------------##########\n",
    "##########-----------------------------<<通用仿浏览器IP头>>----------------------------------------##########\n",
    "##########-------------------------------------------------------------------------------------##########\n",
    "user_agent_pc = [\n",
    "    # 谷歌\n",
    "    'Mozilla/5.0.html (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.html.2171.71 Safari/537.36',\n",
    "    'Mozilla/5.0.html (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.html.1271.64 Safari/537.11',\n",
    "    'Mozilla/5.0.html (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.html.648.133 Safari/534.16',\n",
    "    # 火狐\n",
    "    'Mozilla/5.0.html (Windows NT 6.1; WOW64; rv:34.0.html) Gecko/20100101 Firefox/34.0.html',\n",
    "    'Mozilla/5.0.html (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10',\n",
    "    # opera\n",
    "    'Mozilla/5.0.html (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.html.2171.95 Safari/537.36 OPR/26.0.html.1656.60',\n",
    "    # qq浏览器\n",
    "    'Mozilla/5.0.html (compatible; MSIE 9.0.html; Windows NT 6.1; WOW64; Trident/5.0.html; SLCC2; .NET CLR 2.0.html.50727; .NET CLR 3.5.30729; .NET CLR 3.0.html.30729; Media Center PC 6.0.html; .NET4.0C; .NET4.0E; QQBrowser/7.0.html.3698.400)',\n",
    "    # 搜狗浏览器\n",
    "    'Mozilla/5.0.html (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.html.963.84 Safari/535.11 SE 2.X MetaSr 1.0.html',\n",
    "    # 360浏览器\n",
    "    'Mozilla/5.0.html (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.html.1599.101 Safari/537.36',\n",
    "    'Mozilla/5.0.html (Windows NT 6.1; WOW64; Trident/7.0.html; rv:11.0.html) like Gecko',\n",
    "    # uc浏览器\n",
    "    'Mozilla/5.0.html (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.html.2125.122 UBrowser/4.0.html.3214.0.html Safari/537.36',\n",
    "]\n",
    "# 移动端的user-agent\n",
    "user_agent_phone = [\n",
    "    # IPhone\n",
    "    'Mozilla/5.0.html (iPhone; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.html.2 Mobile/8J2 Safari/6533.18.5',\n",
    "    # IPAD\n",
    "    'Mozilla/5.0.html (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.html.2 Mobile/8C148 Safari/6533.18.5',\n",
    "    'Mozilla/5.0.html (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.html.2 Mobile/8J2 Safari/6533.18.5',\n",
    "    # Android\n",
    "    'Mozilla/5.0.html (Linux; U; Android 2.2.1; zh-cn; HTC_Wildfire_A3333 Build/FRG83D) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0.html Mobile Safari/533.1',\n",
    "    'Mozilla/5.0.html (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0.html Mobile Safari/533.1',\n",
    "    # QQ浏览器 Android版本\n",
    "    'MQQBrowser/26 Mozilla/5.0.html (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0.html Mobile Safari/533.1',\n",
    "    # Android Opera Mobile\n",
    "    'Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10',\n",
    "    # Android Pad Moto Xoom\n",
    "    'Mozilla/5.0.html (Linux; U; Android 3.0.html; en-us; Xoom Build/HRI39) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0.html Safari/534.13',\n",
    "]\n",
    "\n",
    "def get_user_agent_pc():\n",
    "    return random.choice(user_agent_pc)\n",
    "\n",
    "def get_user_agent_phone():\n",
    "    return random.choice(user_agent_phone)\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# -----------------加载网页-----------------------------------------------------------------\n",
    "\n",
    "def fetchUrl(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'user-agent': get_user_agent_pc()\n",
    "        }\n",
    "        r = requests.get(url,headers=headers)\n",
    "        r.encoding = \"gb2312\"\n",
    "    #     print(r)\n",
    "    #     r.raise_for_status()\n",
    "    #     r.encoding = r.apparent_encoding\n",
    "    except:\n",
    "        r = '0'\n",
    "        return r\n",
    "    return r.text\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ----------------------------获取文件中所有的href------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------\n",
    "def get_url_content(file):\n",
    "    with open(file,encoding = 'utf-8') as fr:\n",
    "        ff = fr.read()\n",
    "    fr.close()\n",
    "    bs = bs4.BeautifulSoup(ff,'html.parser')\n",
    "    pageList = bs.find('div', attrs = {'class': 'fr w800'}).find_all('a')\n",
    "    linkList = []\n",
    "    for page in pageList:\n",
    "        try:\n",
    "            str1 = page['href']\n",
    "        except:\n",
    "            str1 = 'NULL'\n",
    "        if str1[0] == 'h':\n",
    "            link = str1\n",
    "            linkList.append(link)\n",
    "    linkList1 = list(set(linkList))\n",
    "    return linkList1 \n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_content():\n",
    "    str1 = 'K:/download/人民网搜索网页/人民网_民间工艺href页面/'\n",
    "    i = 1\n",
    "    str2 = '.txt'\n",
    "    href_List = []\n",
    "    while 1:\n",
    "\n",
    "        file = str1 + str(i) +str2\n",
    "        linkList = get_url_content(file)\n",
    "        href_List = href_List + linkList\n",
    "#         print(ff)\n",
    "        i+=1\n",
    "        if i ==  54:\n",
    "            print(str(i))\n",
    "            break\n",
    "        print(str(i))\n",
    "    return href_List\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ---------------------------获取网页内容---------------------------------------------------\n",
    "def getContent(html):\n",
    "    print(html)\n",
    "    try:\n",
    "\n",
    "        bsobj = bs4.BeautifulSoup(html,'html.parser')   \n",
    "        title = bsobj.h1.text + '\\n'   \n",
    "        pList = bsobj.find_all('p')\n",
    "        content = ''\n",
    "        for p in pList:\n",
    "            content += p.text + '\\n'      \n",
    "    except:\n",
    "        \n",
    "        title='失败，'\n",
    "        content='非人民网类型网页！'\n",
    "    \n",
    "    resp = title + content\n",
    "    return resp\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ------------------------------存储文件---------------------------------------------------\n",
    "\n",
    "def saveFile(content, path, filename):\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(path + filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# -------------------------------下载文件---------------------------------------------------\n",
    "def download_rmrb(destdir):\n",
    "    pageList = get_content()  #  获取文件中所有的href链接 OK\n",
    "#     print(pageList)\n",
    "    i = 1\n",
    "    \n",
    "    for page in pageList:\n",
    "        \n",
    "        html = fetchUrl(page)  #  加载详情网页 \n",
    "        print(html)\n",
    "        content = getContent(html)            \n",
    "        path = destdir + '/' \n",
    "        fileName =str(i) + '.txt'\n",
    "#         print(i)\n",
    "        i=i+1\n",
    "        saveFile(content, path, fileName)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------\n",
    "destdir = 'K:/download/人民网搜索网页/新闻内容'\n",
    "# destdir = \"K:\\download\"\n",
    "download_rmrb(destdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "list1 = [1, 2, 3]\n",
    "\n",
    "list2 = [4, 5, 6]\n",
    "\n",
    "list1 = list1 + list2\n",
    "\n",
    "print(len(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "list1 = [1, 2, 3]\n",
    "\n",
    "list2 = [4, 5, 6]\n",
    "\n",
    "list1 = list1.extend(list2)\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% ####################################################################################################"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for i in range(101):\n",
    "    s=\"\\r%d%% %s\"%(i,\"#\"*i)   #\\r表示回车但是不换行，利用这个原理进行百分比的刷新\n",
    "    sys.stdout.write(s)       #向标准输出终端写内容\n",
    "    sys.stdout.flush()        #立即将缓存的内容刷新到标准输出\n",
    "#     import time\n",
    "#     time.sleep(0.1)           #设置延迟查看效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "\n",
    "def get_filename(num):\n",
    "    str1 = 'K:/download/人民网搜索网页/新闻内容/'\n",
    "    i = num\n",
    "    str2 = '.txt'\n",
    "    filename =  str1 + str(i) +str2\n",
    "#     print(filename)\n",
    "    return filename\n",
    "\n",
    "splitedStr = ''\n",
    "segments = []\n",
    "i = 1\n",
    "\n",
    "w = 0\n",
    "y = 0\n",
    "\n",
    "while i < 2608:\n",
    "    file = get_filename(i)    \n",
    "    with open(file,encoding='utf-8') as f:\n",
    "        content = f.read() \n",
    "    if content[0] == '4' or content[0] == '失':\n",
    "        w = w + 1\n",
    "    else :\n",
    "        words  = jieba.analyse.textrank(content, topK=10,withWeight=False,allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "        for word in words :\n",
    "            # 记录全局分词\n",
    "            segments.append({'word':word, 'count':1})\n",
    "            splitedStr += word + ' '\n",
    "            print('分析第' + str(i) + '篇文章，完成！',end = '\\r')\n",
    "        y = y +1\n",
    "    i= i + 1      \n",
    "    f.close()\n",
    "    \n",
    "dfSg = pd.DataFrame(segments)\n",
    "# 词频统计\n",
    "dfWord = dfSg.groupby('word')['count'].sum()\n",
    "#导出csv\n",
    "dfWord.to_csv('K:/download/人民网搜索网页/词10Key.csv',encoding='utf-8')\n",
    "print('有效文件'+str(y)+'篇！')\n",
    "print('无效效文件'+str(w)+'篇！')\n",
    "print('检索完成！')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
